{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4399ceb4",
   "metadata": {},
   "source": [
    "## Reddit dataset consolidation\n",
    "\n",
    "This notebook gathers every CSV inside `Reddit Dataset/` (except the large `kaggle_RC_2019-05.csv`) and loads them with the correct headers provided in `headers.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "933926a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T04:50:13.711539Z",
     "iopub.status.busy": "2025-11-09T04:50:13.711095Z",
     "iopub.status.idle": "2025-11-09T04:50:14.066467Z",
     "shell.execute_reply": "2025-11-09T04:50:14.065153Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_989515/3918057918.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "/usr/local/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 49 subreddit CSV files to combine.\n",
      "Meta groups: ['entertainment', 'gaming', 'humor', 'learning', 'lifestyle', 'news', 'television']\n",
      "First 5 files: ['entertainment_comicbooks.csv', 'entertainment_harrypotter.csv', 'entertainment_movies.csv', 'entertainment_music.csv', 'entertainment_starwars.csv']\n",
      "Last 5 files: ['television_gameofthrones.csv', 'television_himym.csv', 'television_mylittlepony.csv', 'television_startrek.csv', 'television_thewalkingdead.csv']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "DATA_DIR = Path(\"Reddit Dataset\")\n",
    "EXCLUDE_FILES = {\"kaggle_RC_2019-05.csv\"}  # giant generic dump that dilutes signals\n",
    "\n",
    "COLUMN_NAMES = [\n",
    "    \"text\",\n",
    "    \"id\",\n",
    "    \"subreddit\",\n",
    "    \"meta\",\n",
    "    \"time\",\n",
    "    \"author\",\n",
    "    \"ups\",\n",
    "    \"downs\",\n",
    "    \"authorlinkkarma\",\n",
    "    \"authorkarma\",\n",
    "    \"authorisgold\",\n",
    "]\n",
    "\n",
    "csv_paths = sorted(\n",
    "    path for path in DATA_DIR.glob(\"*.csv\") if path.name not in EXCLUDE_FILES\n",
    ")\n",
    "meta_groups = sorted({path.stem.split(\"_\", 1)[0] for path in csv_paths})\n",
    "\n",
    "print(f\"Found {len(csv_paths)} subreddit CSV files to combine.\")\n",
    "print(\"Meta groups:\", meta_groups)\n",
    "print(\"First 5 files:\", [p.name for p in csv_paths[:5]])\n",
    "print(\"Last 5 files:\", [p.name for p in csv_paths[-5:]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74617f48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T04:50:14.068423Z",
     "iopub.status.busy": "2025-11-09T04:50:14.068111Z",
     "iopub.status.idle": "2025-11-09T04:50:27.170732Z",
     "shell.execute_reply": "2025-11-09T04:50:27.168992Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading subreddit CSVs:   2%|▏         | 1/49 [00:00<00:05,  8.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 1 malformed rows in entertainment_comicbooks.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading subreddit CSVs:   6%|▌         | 3/49 [00:00<00:03, 11.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 1 malformed rows in entertainment_harrypotter.csv\n",
      "Filtered 1 malformed rows in entertainment_movies.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading subreddit CSVs: 100%|██████████| 49/49 [00:04<00:00, 11.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined shape: 2,423,702 rows × 11 columns\n",
      "Total malformed rows dropped: 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>meta</th>\n",
       "      <th>time</th>\n",
       "      <th>author</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>authorlinkkarma</th>\n",
       "      <th>authorkarma</th>\n",
       "      <th>authorisgold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sometimes they have a difference of opinion  s...</td>\n",
       "      <td>d01727e</td>\n",
       "      <td>comicbooks</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>1.455577e+09</td>\n",
       "      <td>TheStealthBox</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>32044.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>try polysuede or felt that is acidfree or pass...</td>\n",
       "      <td>d02fswl</td>\n",
       "      <td>comicbooks</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>1.455661e+09</td>\n",
       "      <td>mrindustrialist</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>take them in to a second hand book store amp  ...</td>\n",
       "      <td>d01qm82</td>\n",
       "      <td>comicbooks</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>1.455615e+09</td>\n",
       "      <td>matthew_lane</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>7710.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a lot of cities have ways of getting comics in...</td>\n",
       "      <td>d01k3vi</td>\n",
       "      <td>comicbooks</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>1.455597e+09</td>\n",
       "      <td>Daiteach</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>439.0</td>\n",
       "      <td>11111.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i m probably in the minority  but even the  wo...</td>\n",
       "      <td>d01km27</td>\n",
       "      <td>comicbooks</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>1.455598e+09</td>\n",
       "      <td>Nejfelt</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>918.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       id   subreddit  \\\n",
       "0  sometimes they have a difference of opinion  s...  d01727e  comicbooks   \n",
       "1  try polysuede or felt that is acidfree or pass...  d02fswl  comicbooks   \n",
       "2  take them in to a second hand book store amp  ...  d01qm82  comicbooks   \n",
       "3  a lot of cities have ways of getting comics in...  d01k3vi  comicbooks   \n",
       "4  i m probably in the minority  but even the  wo...  d01km27  comicbooks   \n",
       "\n",
       "            meta          time           author  ups  downs  authorlinkkarma  \\\n",
       "0  entertainment  1.455577e+09    TheStealthBox  5.0    0.0            208.0   \n",
       "1  entertainment  1.455661e+09  mrindustrialist  1.0    0.0              1.0   \n",
       "2  entertainment  1.455615e+09     matthew_lane  2.0    0.0            250.0   \n",
       "3  entertainment  1.455597e+09         Daiteach  3.0    0.0            439.0   \n",
       "4  entertainment  1.455598e+09          Nejfelt  2.0    0.0            150.0   \n",
       "\n",
       "   authorkarma  authorisgold  \n",
       "0      32044.0           0.0  \n",
       "1         75.0           0.0  \n",
       "2       7710.0           0.0  \n",
       "3      11111.0           0.0  \n",
       "4        918.0           0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = []\n",
    "total_filtered = 0\n",
    "\n",
    "for csv_path in tqdm(csv_paths, desc=\"Loading subreddit CSVs\"):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Drop exported pandas index + rogue placeholder column if present\n",
    "    df = df.drop(columns=df.columns[0])\n",
    "    if len(df.columns) > len(COLUMN_NAMES):\n",
    "        df = df.drop(columns=df.columns[0])\n",
    "\n",
    "    if len(df.columns) != len(COLUMN_NAMES):\n",
    "        raise ValueError(\n",
    "            f\"Unexpected column count {len(df.columns)} in {csv_path.name}.\"\n",
    "        )\n",
    "\n",
    "    df.columns = COLUMN_NAMES\n",
    "\n",
    "    meta_group = csv_path.stem.split(\"_\", 1)[0]\n",
    "    before = len(df)\n",
    "    df = df[df[\"meta\"] == meta_group]\n",
    "    filtered = before - len(df)\n",
    "    if filtered:\n",
    "        total_filtered += filtered\n",
    "        print(f\"Filtered {filtered} malformed rows in {csv_path.name}\")\n",
    "\n",
    "    frames.append(df)\n",
    "\n",
    "combined_df = pd.concat(frames, ignore_index=True)\n",
    "print(f\"\\nCombined shape: {combined_df.shape[0]:,} rows × {combined_df.shape[1]} columns\")\n",
    "print(f\"Total malformed rows dropped: {total_filtered:,}\")\n",
    "combined_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f39abf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T04:50:27.174101Z",
     "iopub.status.busy": "2025-11-09T04:50:27.173703Z",
     "iopub.status.idle": "2025-11-09T04:50:27.503537Z",
     "shell.execute_reply": "2025-11-09T04:50:27.501784Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records per meta subreddit (top 10):\n",
      "meta\n",
      "gaming           428443\n",
      "news             408716\n",
      "lifestyle        384494\n",
      "humor            382197\n",
      "television       321794\n",
      "learning         271179\n",
      "entertainment    226879\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>meta</th>\n",
       "      <th>time</th>\n",
       "      <th>author</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>authorlinkkarma</th>\n",
       "      <th>authorkarma</th>\n",
       "      <th>authorisgold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1767257</th>\n",
       "      <td>i wish this sub would ban dumb shit like this ...</td>\n",
       "      <td>d01yzxb</td>\n",
       "      <td>libertarian</td>\n",
       "      <td>news</td>\n",
       "      <td>1.455638e+09</td>\n",
       "      <td>AlCapone564</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2794.0</td>\n",
       "      <td>1807.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237144</th>\n",
       "      <td>if only mmr could get you attitude</td>\n",
       "      <td>d02kli8</td>\n",
       "      <td>dota2</td>\n",
       "      <td>gaming</td>\n",
       "      <td>1.455668e+09</td>\n",
       "      <td>ShrikeGFX</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>2542.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1747502</th>\n",
       "      <td>so basically you re fucked out of a good job o...</td>\n",
       "      <td>d02tety</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>news</td>\n",
       "      <td>1.455682e+09</td>\n",
       "      <td>goober_boobz</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>2997.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text       id  \\\n",
       "1767257  i wish this sub would ban dumb shit like this ...  d01yzxb   \n",
       "237144                  if only mmr could get you attitude  d02kli8   \n",
       "1747502  so basically you re fucked out of a good job o...  d02tety   \n",
       "\n",
       "           subreddit    meta          time        author   ups  downs  \\\n",
       "1767257  libertarian    news  1.455638e+09   AlCapone564  30.0    0.0   \n",
       "237144         dota2  gaming  1.455668e+09     ShrikeGFX   1.0    0.0   \n",
       "1747502   conspiracy    news  1.455682e+09  goober_boobz   1.0    0.0   \n",
       "\n",
       "         authorlinkkarma  authorkarma  authorisgold  \n",
       "1767257           2794.0       1807.0           0.0  \n",
       "237144             276.0       2542.0           0.0  \n",
       "1747502            190.0       2997.0           0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Records per meta subreddit (top 10):\")\n",
    "print(combined_df[\"meta\"].value_counts().head(10))\n",
    "\n",
    "combined_df.sample(3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "863537a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running embeddings on: CUDA\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running embeddings on: {DEVICE.upper()}\")\n",
    "\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "RECENT_DAYS = 5            # emergence window\n",
    "BASELINE_DAYS = 15         # compare with the preceding period\n",
    "FRESHNESS_DAYS = 20        # ignore tokens seen earlier than this\n",
    "MIN_RECENT_USES = 15       # ensures terms are genuinely active\n",
    "TOP_CANDIDATE_POOL = 120   # rank these many before semantic dedup\n",
    "TARGET_TERM_COUNT = 20\n",
    "COSINE_DUP_THRESHOLD = 0.88\n",
    "\n",
    "TOKEN_REGEX = r\"(?P<token>[a-zA-Z][a-zA-Z0-9'#_+\\-]{1,24})\"\n",
    "\n",
    "STOPWORDS = {\n",
    "    \"the\",\"and\",\"you\",\"that\",\"with\",\"this\",\"have\",\"your\",\"from\",\"they\",\"them\",\n",
    "    \"what\",\"when\",\"were\",\"would\",\"there\",\"could\",\"should\",\"about\",\"because\",\n",
    "    \"their\",\"just\",\"like\",\"cant\",\"dont\",\"doesnt\",\"im\",\"ive\",\"ill\",\"lets\",\n",
    "    \"was\",\"for\",\"are\",\"but\",\n",
    "}\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "617d9951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered to 2,183,806 rows within [2016-01-29 → 2016-02-17].\n",
      "Extracted 59,458,099 token-context rows (60,070 unique tokens).\n"
     ]
    }
   ],
   "source": [
    "df = combined_df.loc[:, [\"meta\", \"time\", \"text\"]].copy()\n",
    "df[\"event_dt\"] = pd.to_datetime(df[\"time\"], unit=\"s\", utc=True).dt.floor(\"D\")\n",
    "\n",
    "analysis_end = df[\"event_dt\"].max()\n",
    "recent_start = analysis_end - pd.Timedelta(days=RECENT_DAYS - 1)\n",
    "baseline_start = recent_start - pd.Timedelta(days=BASELINE_DAYS)\n",
    "fresh_cutoff = analysis_end - pd.Timedelta(days=FRESHNESS_DAYS)\n",
    "\n",
    "window_mask = df[\"event_dt\"].between(baseline_start, analysis_end)\n",
    "df = df.loc[window_mask].copy()\n",
    "df[\"text_norm\"] = df[\"text\"].fillna(\"\").map(normalize_text)\n",
    "df = df.loc[df[\"text_norm\"].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "print(f\"Filtered to {len(df):,} rows within [{baseline_start.date()} → {analysis_end.date()}].\")\n",
    "\n",
    "token_matches = (\n",
    "    df[\"text_norm\"]\n",
    "    .str.extractall(TOKEN_REGEX)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"level_0\": \"row_idx\", \"token\": \"token\"})\n",
    ")\n",
    "\n",
    "token_df = token_matches.merge(\n",
    "    df[[\"meta\", \"event_dt\", \"text_norm\"]],\n",
    "    left_on=\"row_idx\",\n",
    "    right_index=True,\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "token_df = token_df.loc[~token_df[\"token\"].isin(STOPWORDS)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Extracted {len(token_df):,} token-context rows \"\n",
    "      f\"({token_df['token'].nunique():,} unique tokens).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee780bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate pool after filters: 111075 token/meta pairs.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meta</th>\n",
       "      <th>token</th>\n",
       "      <th>recent_freq</th>\n",
       "      <th>baseline_freq</th>\n",
       "      <th>first_seen</th>\n",
       "      <th>last_seen</th>\n",
       "      <th>example_context</th>\n",
       "      <th>growth_ratio</th>\n",
       "      <th>novelty_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43065</th>\n",
       "      <td>humor</td>\n",
       "      <td>sanders</td>\n",
       "      <td>188700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-02-13 00:00:00+00:00</td>\n",
       "      <td>2016-02-17 00:00:00+00:00</td>\n",
       "      <td>but they were their party was literally called...</td>\n",
       "      <td>188701.0</td>\n",
       "      <td>3.560788e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39397</th>\n",
       "      <td>humor</td>\n",
       "      <td>kanye</td>\n",
       "      <td>77245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-02-14 00:00:00+00:00</td>\n",
       "      <td>2016-02-17 00:00:00+00:00</td>\n",
       "      <td>kanye lost his soul when his moma died</td>\n",
       "      <td>77246.0</td>\n",
       "      <td>5.966867e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45786</th>\n",
       "      <td>humor</td>\n",
       "      <td>west</td>\n",
       "      <td>74898</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-02-14 00:00:00+00:00</td>\n",
       "      <td>2016-02-17 00:00:00+00:00</td>\n",
       "      <td>here s a link to the story with a video it s i...</td>\n",
       "      <td>74899.0</td>\n",
       "      <td>5.609785e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40243</th>\n",
       "      <td>humor</td>\n",
       "      <td>mentally</td>\n",
       "      <td>74298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-02-16 00:00:00+00:00</td>\n",
       "      <td>2016-02-17 00:00:00+00:00</td>\n",
       "      <td>they basically are children mentally they re t...</td>\n",
       "      <td>74299.0</td>\n",
       "      <td>5.520267e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41241</th>\n",
       "      <td>humor</td>\n",
       "      <td>pablo</td>\n",
       "      <td>67649</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-02-15 00:00:00+00:00</td>\n",
       "      <td>2016-02-17 00:00:00+00:00</td>\n",
       "      <td>downpablo if you must but i am going to listen...</td>\n",
       "      <td>67650.0</td>\n",
       "      <td>4.576455e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        meta     token  recent_freq  baseline_freq                first_seen  \\\n",
       "43065  humor   sanders       188700            0.0 2016-02-13 00:00:00+00:00   \n",
       "39397  humor     kanye        77245            0.0 2016-02-14 00:00:00+00:00   \n",
       "45786  humor      west        74898            0.0 2016-02-14 00:00:00+00:00   \n",
       "40243  humor  mentally        74298            0.0 2016-02-16 00:00:00+00:00   \n",
       "41241  humor     pablo        67649            0.0 2016-02-15 00:00:00+00:00   \n",
       "\n",
       "                      last_seen  \\\n",
       "43065 2016-02-17 00:00:00+00:00   \n",
       "39397 2016-02-17 00:00:00+00:00   \n",
       "45786 2016-02-17 00:00:00+00:00   \n",
       "40243 2016-02-17 00:00:00+00:00   \n",
       "41241 2016-02-17 00:00:00+00:00   \n",
       "\n",
       "                                         example_context  growth_ratio  \\\n",
       "43065  but they were their party was literally called...      188701.0   \n",
       "39397             kanye lost his soul when his moma died       77246.0   \n",
       "45786  here s a link to the story with a video it s i...       74899.0   \n",
       "40243  they basically are children mentally they re t...       74299.0   \n",
       "41241  downpablo if you must but i am going to listen...       67650.0   \n",
       "\n",
       "       novelty_score  \n",
       "43065   3.560788e+10  \n",
       "39397   5.966867e+09  \n",
       "45786   5.609785e+09  \n",
       "40243   5.520267e+09  \n",
       "41241   4.576455e+09  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recent_mask = token_df[\"event_dt\"] >= recent_start\n",
    "baseline_mask = token_df[\"event_dt\"].between(baseline_start, recent_start - pd.Timedelta(days=1))\n",
    "fresh_mask = token_df[\"event_dt\"] >= fresh_cutoff\n",
    "\n",
    "recent_counts = (\n",
    "    token_df.loc[recent_mask]\n",
    "    .groupby([\"meta\", \"token\"])\n",
    "    .size()\n",
    "    .rename(\"recent_freq\")\n",
    ")\n",
    "\n",
    "baseline_counts = (\n",
    "    token_df.loc[baseline_mask]\n",
    "    .groupby([\"meta\", \"token\"])\n",
    "    .size()\n",
    "    .rename(\"baseline_freq\")\n",
    ")\n",
    "\n",
    "first_seen = (\n",
    "    token_df.groupby([\"meta\", \"token\"])[\"event_dt\"]\n",
    "    .min()\n",
    "    .rename(\"first_seen\")\n",
    ")\n",
    "\n",
    "last_context = (\n",
    "    token_df.loc[recent_mask]\n",
    "    .sort_values(\"event_dt\")\n",
    "    .groupby([\"meta\", \"token\"])\n",
    "    .agg(\n",
    "        last_seen=(\"event_dt\", \"max\"),\n",
    "        example_context=(\"text_norm\", \"last\")\n",
    "    )\n",
    ")\n",
    "\n",
    "candidate_stats = (\n",
    "    recent_counts.to_frame()\n",
    "    .join(baseline_counts, how=\"left\")\n",
    "    .join(first_seen, how=\"left\")\n",
    "    .join(last_context, how=\"left\")\n",
    "    .fillna({\"baseline_freq\": 0})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "candidate_stats = candidate_stats[\n",
    "    (candidate_stats[\"recent_freq\"] >= MIN_RECENT_USES) &\n",
    "    (candidate_stats[\"first_seen\"] >= fresh_cutoff)\n",
    "]\n",
    "\n",
    "candidate_stats[\"growth_ratio\"] = (candidate_stats[\"recent_freq\"] + 1) / (candidate_stats[\"baseline_freq\"] + 1)\n",
    "candidate_stats[\"novelty_score\"] = candidate_stats[\"recent_freq\"] * candidate_stats[\"growth_ratio\"]\n",
    "\n",
    "candidate_stats = candidate_stats.sort_values(\"novelty_score\", ascending=False)\n",
    "\n",
    "print(f\"Candidate pool after filters: {len(candidate_stats)} token/meta pairs.\")\n",
    "candidate_stats.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f4e06c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.52it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meta</th>\n",
       "      <th>token</th>\n",
       "      <th>recent_freq</th>\n",
       "      <th>baseline_freq</th>\n",
       "      <th>growth_ratio</th>\n",
       "      <th>novelty_score</th>\n",
       "      <th>first_seen</th>\n",
       "      <th>last_seen</th>\n",
       "      <th>example_context</th>\n",
       "      <th>baseline_window</th>\n",
       "      <th>recent_window</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>humor</td>\n",
       "      <td>sanders</td>\n",
       "      <td>188700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>188701.000000</td>\n",
       "      <td>3.560788e+10</td>\n",
       "      <td>2016-02-13 00:00:00+00:00</td>\n",
       "      <td>2016-02-17 00:00:00+00:00</td>\n",
       "      <td>but they were their party was literally called...</td>\n",
       "      <td>2016-01-29 → 2016-02-12</td>\n",
       "      <td>2016-02-13 → 2016-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>humor</td>\n",
       "      <td>kanye</td>\n",
       "      <td>77245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>77246.000000</td>\n",
       "      <td>5.966867e+09</td>\n",
       "      <td>2016-02-14 00:00:00+00:00</td>\n",
       "      <td>2016-02-17 00:00:00+00:00</td>\n",
       "      <td>kanye lost his soul when his moma died</td>\n",
       "      <td>2016-01-29 → 2016-02-12</td>\n",
       "      <td>2016-02-13 → 2016-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>humor</td>\n",
       "      <td>west</td>\n",
       "      <td>74898</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74899.000000</td>\n",
       "      <td>5.609785e+09</td>\n",
       "      <td>2016-02-14 00:00:00+00:00</td>\n",
       "      <td>2016-02-17 00:00:00+00:00</td>\n",
       "      <td>here s a link to the story with a video it s i...</td>\n",
       "      <td>2016-01-29 → 2016-02-12</td>\n",
       "      <td>2016-02-13 → 2016-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>humor</td>\n",
       "      <td>mentally</td>\n",
       "      <td>74298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74299.000000</td>\n",
       "      <td>5.520267e+09</td>\n",
       "      <td>2016-02-16 00:00:00+00:00</td>\n",
       "      <td>2016-02-17 00:00:00+00:00</td>\n",
       "      <td>they basically are children mentally they re t...</td>\n",
       "      <td>2016-01-29 → 2016-02-12</td>\n",
       "      <td>2016-02-13 → 2016-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>humor</td>\n",
       "      <td>pablo</td>\n",
       "      <td>67649</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67650.000000</td>\n",
       "      <td>4.576455e+09</td>\n",
       "      <td>2016-02-15 00:00:00+00:00</td>\n",
       "      <td>2016-02-17 00:00:00+00:00</td>\n",
       "      <td>downpablo if you must but i am going to listen...</td>\n",
       "      <td>2016-01-29 → 2016-02-12</td>\n",
       "      <td>2016-02-13 → 2016-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>humor</td>\n",
       "      <td>ber</td>\n",
       "      <td>62550</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62551.000000</td>\n",
       "      <td>3.912565e+09</td>\n",
       "      <td>2016-02-14 00:00:00+00:00</td>\n",
       "      <td>2016-02-17 00:00:00+00:00</td>\n",
       "      <td>ber nie ber nie ber nie ber nie ber nie ber ni...</td>\n",
       "      <td>2016-01-29 → 2016-02-12</td>\n",
       "      <td>2016-02-13 → 2016-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>humor</td>\n",
       "      <td>bernie</td>\n",
       "      <td>191947</td>\n",
       "      <td>50.0</td>\n",
       "      <td>3763.686275</td>\n",
       "      <td>7.224283e+08</td>\n",
       "      <td>2016-02-12 00:00:00+00:00</td>\n",
       "      <td>2016-02-17 00:00:00+00:00</td>\n",
       "      <td>i had to setup bernie his own deskbed so he wo...</td>\n",
       "      <td>2016-01-29 → 2016-02-12</td>\n",
       "      <td>2016-02-13 → 2016-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lifestyle</td>\n",
       "      <td>bike</td>\n",
       "      <td>26669</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26670.000000</td>\n",
       "      <td>7.112622e+08</td>\n",
       "      <td>2016-02-13 00:00:00+00:00</td>\n",
       "      <td>2016-02-17 00:00:00+00:00</td>\n",
       "      <td>this was my experience shopping for ducs and b...</td>\n",
       "      <td>2016-01-29 → 2016-02-12</td>\n",
       "      <td>2016-02-13 → 2016-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>humor</td>\n",
       "      <td>time</td>\n",
       "      <td>18972</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18973.000000</td>\n",
       "      <td>3.599558e+08</td>\n",
       "      <td>2016-02-13 00:00:00+00:00</td>\n",
       "      <td>2016-02-17 00:00:00+00:00</td>\n",
       "      <td>ugh these things always make me cringe but eve...</td>\n",
       "      <td>2016-01-29 → 2016-02-12</td>\n",
       "      <td>2016-02-13 → 2016-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>news</td>\n",
       "      <td>bernie</td>\n",
       "      <td>17583</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17584.000000</td>\n",
       "      <td>3.091795e+08</td>\n",
       "      <td>2016-02-13 00:00:00+00:00</td>\n",
       "      <td>2016-02-17 00:00:00+00:00</td>\n",
       "      <td>kind of sounds like what bernie sanders has be...</td>\n",
       "      <td>2016-01-29 → 2016-02-12</td>\n",
       "      <td>2016-02-13 → 2016-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gaming</td>\n",
       "      <td>team</td>\n",
       "      <td>15993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15994.000000</td>\n",
       "      <td>2.557920e+08</td>\n",
       "      <td>2016-02-13 00:00:00+00:00</td>\n",
       "      <td>2016-02-17 00:00:00+00:00</td>\n",
       "      <td>had to look up the cipher wheel quick i ve see...</td>\n",
       "      <td>2016-01-29 → 2016-02-12</td>\n",
       "      <td>2016-02-13 → 2016-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gaming</td>\n",
       "      <td>pokemon</td>\n",
       "      <td>15783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15784.000000</td>\n",
       "      <td>2.491189e+08</td>\n",
       "      <td>2016-02-15 00:00:00+00:00</td>\n",
       "      <td>2016-02-17 00:00:00+00:00</td>\n",
       "      <td>damn pokemon is turning into power rangers jus...</td>\n",
       "      <td>2016-01-29 → 2016-02-12</td>\n",
       "      <td>2016-02-13 → 2016-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>gaming</td>\n",
       "      <td>jakiro</td>\n",
       "      <td>15299</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15300.000000</td>\n",
       "      <td>2.340747e+08</td>\n",
       "      <td>2016-02-15 00:00:00+00:00</td>\n",
       "      <td>2016-02-17 00:00:00+00:00</td>\n",
       "      <td>gt i ve held off playing ranked because i thin...</td>\n",
       "      <td>2016-01-29 → 2016-02-12</td>\n",
       "      <td>2016-02-13 → 2016-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>humor</td>\n",
       "      <td>good</td>\n",
       "      <td>14618</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>2.137005e+08</td>\n",
       "      <td>2016-02-13 00:00:00+00:00</td>\n",
       "      <td>2016-02-17 00:00:00+00:00</td>\n",
       "      <td>an engineer a physicist a mathematician and a ...</td>\n",
       "      <td>2016-01-29 → 2016-02-12</td>\n",
       "      <td>2016-02-13 → 2016-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>news</td>\n",
       "      <td>hillary</td>\n",
       "      <td>14373</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14374.000000</td>\n",
       "      <td>2.065975e+08</td>\n",
       "      <td>2016-02-13 00:00:00+00:00</td>\n",
       "      <td>2016-02-17 00:00:00+00:00</td>\n",
       "      <td>notice that hackers never dox the 01 billionai...</td>\n",
       "      <td>2016-01-29 → 2016-02-12</td>\n",
       "      <td>2016-02-13 → 2016-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>album</td>\n",
       "      <td>11564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11565.000000</td>\n",
       "      <td>1.337377e+08</td>\n",
       "      <td>2016-02-13 00:00:00+00:00</td>\n",
       "      <td>2016-02-17 00:00:00+00:00</td>\n",
       "      <td>noticing a lot of dudes in the main list so a ...</td>\n",
       "      <td>2016-01-29 → 2016-02-12</td>\n",
       "      <td>2016-02-13 → 2016-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>humor</td>\n",
       "      <td>back</td>\n",
       "      <td>11440</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11441.000000</td>\n",
       "      <td>1.308850e+08</td>\n",
       "      <td>2016-02-13 00:00:00+00:00</td>\n",
       "      <td>2016-02-17 00:00:00+00:00</td>\n",
       "      <td>the metaphor being the computer wo nt turn on ...</td>\n",
       "      <td>2016-01-29 → 2016-02-12</td>\n",
       "      <td>2016-02-13 → 2016-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>harry</td>\n",
       "      <td>11331</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11332.000000</td>\n",
       "      <td>1.284029e+08</td>\n",
       "      <td>2016-02-13 00:00:00+00:00</td>\n",
       "      <td>2016-02-17 00:00:00+00:00</td>\n",
       "      <td>in deathly hallows part 1 when dobby is dying ...</td>\n",
       "      <td>2016-01-29 → 2016-02-12</td>\n",
       "      <td>2016-02-13 → 2016-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>gaming</td>\n",
       "      <td>shit</td>\n",
       "      <td>11153</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11154.000000</td>\n",
       "      <td>1.244006e+08</td>\n",
       "      <td>2016-02-13 00:00:00+00:00</td>\n",
       "      <td>2016-02-17 00:00:00+00:00</td>\n",
       "      <td>nice work op it would be interesting to gather...</td>\n",
       "      <td>2016-01-29 → 2016-02-12</td>\n",
       "      <td>2016-02-13 → 2016-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>news</td>\n",
       "      <td>actually</td>\n",
       "      <td>10945</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10946.000000</td>\n",
       "      <td>1.198040e+08</td>\n",
       "      <td>2016-02-13 00:00:00+00:00</td>\n",
       "      <td>2016-02-17 00:00:00+00:00</td>\n",
       "      <td>gt the undergrad s lawsuit says he had sex wit...</td>\n",
       "      <td>2016-01-29 → 2016-02-12</td>\n",
       "      <td>2016-02-13 → 2016-02-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             meta     token  recent_freq  baseline_freq   growth_ratio  \\\n",
       "0           humor   sanders       188700            0.0  188701.000000   \n",
       "1           humor     kanye        77245            0.0   77246.000000   \n",
       "2           humor      west        74898            0.0   74899.000000   \n",
       "3           humor  mentally        74298            0.0   74299.000000   \n",
       "4           humor     pablo        67649            0.0   67650.000000   \n",
       "5           humor       ber        62550            0.0   62551.000000   \n",
       "6           humor    bernie       191947           50.0    3763.686275   \n",
       "7       lifestyle      bike        26669            0.0   26670.000000   \n",
       "8           humor      time        18972            0.0   18973.000000   \n",
       "9            news    bernie        17583            0.0   17584.000000   \n",
       "10         gaming      team        15993            0.0   15994.000000   \n",
       "11         gaming   pokemon        15783            0.0   15784.000000   \n",
       "12         gaming    jakiro        15299            0.0   15300.000000   \n",
       "13          humor      good        14618            0.0   14619.000000   \n",
       "14           news   hillary        14373            0.0   14374.000000   \n",
       "15  entertainment     album        11564            0.0   11565.000000   \n",
       "16          humor      back        11440            0.0   11441.000000   \n",
       "17  entertainment     harry        11331            0.0   11332.000000   \n",
       "18         gaming      shit        11153            0.0   11154.000000   \n",
       "19           news  actually        10945            0.0   10946.000000   \n",
       "\n",
       "    novelty_score                first_seen                 last_seen  \\\n",
       "0    3.560788e+10 2016-02-13 00:00:00+00:00 2016-02-17 00:00:00+00:00   \n",
       "1    5.966867e+09 2016-02-14 00:00:00+00:00 2016-02-17 00:00:00+00:00   \n",
       "2    5.609785e+09 2016-02-14 00:00:00+00:00 2016-02-17 00:00:00+00:00   \n",
       "3    5.520267e+09 2016-02-16 00:00:00+00:00 2016-02-17 00:00:00+00:00   \n",
       "4    4.576455e+09 2016-02-15 00:00:00+00:00 2016-02-17 00:00:00+00:00   \n",
       "5    3.912565e+09 2016-02-14 00:00:00+00:00 2016-02-17 00:00:00+00:00   \n",
       "6    7.224283e+08 2016-02-12 00:00:00+00:00 2016-02-17 00:00:00+00:00   \n",
       "7    7.112622e+08 2016-02-13 00:00:00+00:00 2016-02-17 00:00:00+00:00   \n",
       "8    3.599558e+08 2016-02-13 00:00:00+00:00 2016-02-17 00:00:00+00:00   \n",
       "9    3.091795e+08 2016-02-13 00:00:00+00:00 2016-02-17 00:00:00+00:00   \n",
       "10   2.557920e+08 2016-02-13 00:00:00+00:00 2016-02-17 00:00:00+00:00   \n",
       "11   2.491189e+08 2016-02-15 00:00:00+00:00 2016-02-17 00:00:00+00:00   \n",
       "12   2.340747e+08 2016-02-15 00:00:00+00:00 2016-02-17 00:00:00+00:00   \n",
       "13   2.137005e+08 2016-02-13 00:00:00+00:00 2016-02-17 00:00:00+00:00   \n",
       "14   2.065975e+08 2016-02-13 00:00:00+00:00 2016-02-17 00:00:00+00:00   \n",
       "15   1.337377e+08 2016-02-13 00:00:00+00:00 2016-02-17 00:00:00+00:00   \n",
       "16   1.308850e+08 2016-02-13 00:00:00+00:00 2016-02-17 00:00:00+00:00   \n",
       "17   1.284029e+08 2016-02-13 00:00:00+00:00 2016-02-17 00:00:00+00:00   \n",
       "18   1.244006e+08 2016-02-13 00:00:00+00:00 2016-02-17 00:00:00+00:00   \n",
       "19   1.198040e+08 2016-02-13 00:00:00+00:00 2016-02-17 00:00:00+00:00   \n",
       "\n",
       "                                      example_context  \\\n",
       "0   but they were their party was literally called...   \n",
       "1              kanye lost his soul when his moma died   \n",
       "2   here s a link to the story with a video it s i...   \n",
       "3   they basically are children mentally they re t...   \n",
       "4   downpablo if you must but i am going to listen...   \n",
       "5   ber nie ber nie ber nie ber nie ber nie ber ni...   \n",
       "6   i had to setup bernie his own deskbed so he wo...   \n",
       "7   this was my experience shopping for ducs and b...   \n",
       "8   ugh these things always make me cringe but eve...   \n",
       "9   kind of sounds like what bernie sanders has be...   \n",
       "10  had to look up the cipher wheel quick i ve see...   \n",
       "11  damn pokemon is turning into power rangers jus...   \n",
       "12  gt i ve held off playing ranked because i thin...   \n",
       "13  an engineer a physicist a mathematician and a ...   \n",
       "14  notice that hackers never dox the 01 billionai...   \n",
       "15  noticing a lot of dudes in the main list so a ...   \n",
       "16  the metaphor being the computer wo nt turn on ...   \n",
       "17  in deathly hallows part 1 when dobby is dying ...   \n",
       "18  nice work op it would be interesting to gather...   \n",
       "19  gt the undergrad s lawsuit says he had sex wit...   \n",
       "\n",
       "            baseline_window            recent_window  \n",
       "0   2016-01-29 → 2016-02-12  2016-02-13 → 2016-02-17  \n",
       "1   2016-01-29 → 2016-02-12  2016-02-13 → 2016-02-17  \n",
       "2   2016-01-29 → 2016-02-12  2016-02-13 → 2016-02-17  \n",
       "3   2016-01-29 → 2016-02-12  2016-02-13 → 2016-02-17  \n",
       "4   2016-01-29 → 2016-02-12  2016-02-13 → 2016-02-17  \n",
       "5   2016-01-29 → 2016-02-12  2016-02-13 → 2016-02-17  \n",
       "6   2016-01-29 → 2016-02-12  2016-02-13 → 2016-02-17  \n",
       "7   2016-01-29 → 2016-02-12  2016-02-13 → 2016-02-17  \n",
       "8   2016-01-29 → 2016-02-12  2016-02-13 → 2016-02-17  \n",
       "9   2016-01-29 → 2016-02-12  2016-02-13 → 2016-02-17  \n",
       "10  2016-01-29 → 2016-02-12  2016-02-13 → 2016-02-17  \n",
       "11  2016-01-29 → 2016-02-12  2016-02-13 → 2016-02-17  \n",
       "12  2016-01-29 → 2016-02-12  2016-02-13 → 2016-02-17  \n",
       "13  2016-01-29 → 2016-02-12  2016-02-13 → 2016-02-17  \n",
       "14  2016-01-29 → 2016-02-12  2016-02-13 → 2016-02-17  \n",
       "15  2016-01-29 → 2016-02-12  2016-02-13 → 2016-02-17  \n",
       "16  2016-01-29 → 2016-02-12  2016-02-13 → 2016-02-17  \n",
       "17  2016-01-29 → 2016-02-12  2016-02-13 → 2016-02-17  \n",
       "18  2016-01-29 → 2016-02-12  2016-02-13 → 2016-02-17  \n",
       "19  2016-01-29 → 2016-02-12  2016-02-13 → 2016-02-17  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Returned 20 candidate terms ready for SIR modeling, semantic tracking, and downstream analysis.\n"
     ]
    }
   ],
   "source": [
    "if candidate_stats.empty:\n",
    "    print(\"❌ No terms satisfy the frequency/freshness criteria. \"\n",
    "          \"Relax the thresholds and re-run.\")\n",
    "else:\n",
    "    pool = candidate_stats.head(TOP_CANDIDATE_POOL).copy()\n",
    "    pool[\"context_for_embed\"] = pool[\"token\"] + \" || \" + pool[\"example_context\"].fillna(\"\")\n",
    "\n",
    "    model = SentenceTransformer(MODEL_NAME, device=DEVICE)\n",
    "    embeddings = model.encode(\n",
    "        pool[\"context_for_embed\"].tolist(),\n",
    "        batch_size=256,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,\n",
    "    )\n",
    "    pool[\"embedding\"] = list(embeddings)\n",
    "\n",
    "    def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:\n",
    "        return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-9))\n",
    "\n",
    "    selected_rows = []\n",
    "    for idx, row in pool.iterrows():\n",
    "        emb = row[\"embedding\"]\n",
    "        if all(cosine_sim(emb, sel[\"embedding\"]) < COSINE_DUP_THRESHOLD for sel in selected_rows):\n",
    "            selected_rows.append(row)\n",
    "        if len(selected_rows) == TARGET_TERM_COUNT:\n",
    "            break\n",
    "\n",
    "    if len(selected_rows) < TARGET_TERM_COUNT:\n",
    "        print(f\"⚠️ Only {len(selected_rows)} unique terms after dedup; \"\n",
    "              f\"no more high-quality candidates available.\")\n",
    "        selected_rows = selected_rows or pool.head(TARGET_TERM_COUNT).to_dict(\"records\")\n",
    "\n",
    "    candidates = (\n",
    "        pd.DataFrame(selected_rows)\n",
    "          .drop(columns=[\"embedding\", \"context_for_embed\"])\n",
    "          .reset_index(drop=True)\n",
    "          .assign(\n",
    "              baseline_window=f\"{baseline_start.date()} → {(recent_start - pd.Timedelta(days=1)).date()}\",\n",
    "              recent_window=f\"{recent_start.date()} → {analysis_end.date()}\",\n",
    "          )\n",
    "    )\n",
    "\n",
    "    display(candidates[[\n",
    "        \"meta\", \"token\", \"recent_freq\", \"baseline_freq\", \"growth_ratio\",\n",
    "        \"novelty_score\", \"first_seen\", \"last_seen\", \"example_context\",\n",
    "        \"baseline_window\", \"recent_window\",\n",
    "    ]])\n",
    "\n",
    "    print(f\"\\nReturned {len(candidates)} candidate terms ready for SIR modeling, \"\n",
    "          \"semantic tracking, and downstream analysis.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11fdce6",
   "metadata": {},
   "source": [
    "The above are 20 candidate terms evaluating using embedding anomaly detection. However, many of these terms continue to be common words rather than true slang; we must try alternative methods to identify evolving terms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
