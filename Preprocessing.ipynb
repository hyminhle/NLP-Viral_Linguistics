{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4399ceb4",
   "metadata": {},
   "source": [
    "## Reddit dataset consolidation\n",
    "\n",
    "This notebook gathers every CSV inside `Reddit Dataset/` (except the large `kaggle_RC_2019-05.csv`) and loads them with the correct headers provided in `headers.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "933926a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T04:50:13.711539Z",
     "iopub.status.busy": "2025-11-09T04:50:13.711095Z",
     "iopub.status.idle": "2025-11-09T04:50:14.066467Z",
     "shell.execute_reply": "2025-11-09T04:50:14.065153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 49 subreddit CSV files to combine.\n",
      "Meta groups: ['entertainment', 'gaming', 'humor', 'learning', 'lifestyle', 'news', 'television']\n",
      "First 5 files: ['entertainment_comicbooks.csv', 'entertainment_harrypotter.csv', 'entertainment_movies.csv', 'entertainment_music.csv', 'entertainment_starwars.csv']\n",
      "Last 5 files: ['television_gameofthrones.csv', 'television_himym.csv', 'television_mylittlepony.csv', 'television_startrek.csv', 'television_thewalkingdead.csv']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = Path('Reddit Dataset')\n",
    "COLUMN_NAMES = [\n",
    "    'text',\n",
    "    'id',\n",
    "    'subreddit',\n",
    "    'meta',\n",
    "    'time',\n",
    "    'author',\n",
    "    'ups',\n",
    "    'downs',\n",
    "    'authorlinkkarma',\n",
    "    'authorkarma',\n",
    "    'authorisgold',\n",
    "]\n",
    "\n",
    "csv_paths = sorted(\n",
    "    path for path in DATA_DIR.glob('*.csv')\n",
    ")\n",
    "meta_groups = sorted({path.stem.split('_', 1)[0] for path in csv_paths})\n",
    "print(f\"Found {len(csv_paths)} subreddit CSV files to combine.\")\n",
    "print('Meta groups:', meta_groups)\n",
    "print('First 5 files:', [p.name for p in csv_paths[:5]])\n",
    "print('Last 5 files:', [p.name for p in csv_paths[-5:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74617f48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T04:50:14.068423Z",
     "iopub.status.busy": "2025-11-09T04:50:14.068111Z",
     "iopub.status.idle": "2025-11-09T04:50:27.170732Z",
     "shell.execute_reply": "2025-11-09T04:50:27.168992Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 1 malformed rows in entertainment_comicbooks.csv\n",
      "Filtered 1 malformed rows in entertainment_harrypotter.csv\n",
      "Filtered 1 malformed rows in entertainment_movies.csv\n",
      "Combined shape: 2,423,702 rows x 11 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>meta</th>\n",
       "      <th>time</th>\n",
       "      <th>author</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>authorlinkkarma</th>\n",
       "      <th>authorkarma</th>\n",
       "      <th>authorisgold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sometimes they have a difference of opinion  s...</td>\n",
       "      <td>d01727e</td>\n",
       "      <td>comicbooks</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>1.455577e+09</td>\n",
       "      <td>TheStealthBox</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>32044.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>try polysuede or felt that is acidfree or pass...</td>\n",
       "      <td>d02fswl</td>\n",
       "      <td>comicbooks</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>1.455661e+09</td>\n",
       "      <td>mrindustrialist</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>take them in to a second hand book store amp  ...</td>\n",
       "      <td>d01qm82</td>\n",
       "      <td>comicbooks</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>1.455615e+09</td>\n",
       "      <td>matthew_lane</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>7710.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a lot of cities have ways of getting comics in...</td>\n",
       "      <td>d01k3vi</td>\n",
       "      <td>comicbooks</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>1.455597e+09</td>\n",
       "      <td>Daiteach</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>439.0</td>\n",
       "      <td>11111.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i m probably in the minority  but even the  wo...</td>\n",
       "      <td>d01km27</td>\n",
       "      <td>comicbooks</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>1.455598e+09</td>\n",
       "      <td>Nejfelt</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>918.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       id   subreddit  \\\n",
       "0  sometimes they have a difference of opinion  s...  d01727e  comicbooks   \n",
       "1  try polysuede or felt that is acidfree or pass...  d02fswl  comicbooks   \n",
       "2  take them in to a second hand book store amp  ...  d01qm82  comicbooks   \n",
       "3  a lot of cities have ways of getting comics in...  d01k3vi  comicbooks   \n",
       "4  i m probably in the minority  but even the  wo...  d01km27  comicbooks   \n",
       "\n",
       "            meta          time           author  ups  downs  authorlinkkarma  \\\n",
       "0  entertainment  1.455577e+09    TheStealthBox  5.0    0.0            208.0   \n",
       "1  entertainment  1.455661e+09  mrindustrialist  1.0    0.0              1.0   \n",
       "2  entertainment  1.455615e+09     matthew_lane  2.0    0.0            250.0   \n",
       "3  entertainment  1.455597e+09         Daiteach  3.0    0.0            439.0   \n",
       "4  entertainment  1.455598e+09          Nejfelt  2.0    0.0            150.0   \n",
       "\n",
       "   authorkarma  authorisgold  \n",
       "0      32044.0           0.0  \n",
       "1         75.0           0.0  \n",
       "2       7710.0           0.0  \n",
       "3      11111.0           0.0  \n",
       "4        918.0           0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = []\n",
    "total_filtered = 0\n",
    "for csv_path in csv_paths:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.drop(columns=df.columns[0])\n",
    "    if len(df.columns) > len(COLUMN_NAMES):\n",
    "        df = df.drop(columns=df.columns[0])\n",
    "    if len(df.columns) != len(COLUMN_NAMES):\n",
    "        raise ValueError(\n",
    "            f\"Unexpected column count {len(df.columns)} in {csv_path.name}.\"\n",
    "        )\n",
    "    df.columns = COLUMN_NAMES\n",
    "    meta_group = csv_path.stem.split('_', 1)[0]\n",
    "    before = len(df)\n",
    "    df = df[df['meta'] == meta_group]\n",
    "    filtered = before - len(df)\n",
    "    if filtered:\n",
    "        total_filtered += filtered\n",
    "        print(f\"Filtered {filtered} malformed rows in {csv_path.name}\")\n",
    "    frames.append(df)\n",
    "\n",
    "combined_df = pd.concat(frames, ignore_index=True)\n",
    "print(f\"Combined shape: {combined_df.shape[0]:,} rows x {combined_df.shape[1]} columns\")\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f39abf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T04:50:27.174101Z",
     "iopub.status.busy": "2025-11-09T04:50:27.173703Z",
     "iopub.status.idle": "2025-11-09T04:50:27.503537Z",
     "shell.execute_reply": "2025-11-09T04:50:27.501784Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records per meta subreddit (top 10):\n",
      "meta\n",
      "gaming           428443\n",
      "news             408716\n",
      "lifestyle        384494\n",
      "humor            382197\n",
      "television       321794\n",
      "learning         271179\n",
      "entertainment    226879\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>meta</th>\n",
       "      <th>time</th>\n",
       "      <th>author</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>authorlinkkarma</th>\n",
       "      <th>authorkarma</th>\n",
       "      <th>authorisgold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1767257</th>\n",
       "      <td>i wish this sub would ban dumb shit like this ...</td>\n",
       "      <td>d01yzxb</td>\n",
       "      <td>libertarian</td>\n",
       "      <td>news</td>\n",
       "      <td>1.455638e+09</td>\n",
       "      <td>AlCapone564</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2794.0</td>\n",
       "      <td>1807.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237144</th>\n",
       "      <td>if only mmr could get you attitude</td>\n",
       "      <td>d02kli8</td>\n",
       "      <td>dota2</td>\n",
       "      <td>gaming</td>\n",
       "      <td>1.455668e+09</td>\n",
       "      <td>ShrikeGFX</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>2542.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1747502</th>\n",
       "      <td>so basically you re fucked out of a good job o...</td>\n",
       "      <td>d02tety</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>news</td>\n",
       "      <td>1.455682e+09</td>\n",
       "      <td>goober_boobz</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>2997.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text       id  \\\n",
       "1767257  i wish this sub would ban dumb shit like this ...  d01yzxb   \n",
       "237144                  if only mmr could get you attitude  d02kli8   \n",
       "1747502  so basically you re fucked out of a good job o...  d02tety   \n",
       "\n",
       "           subreddit    meta          time        author   ups  downs  \\\n",
       "1767257  libertarian    news  1.455638e+09   AlCapone564  30.0    0.0   \n",
       "237144         dota2  gaming  1.455668e+09     ShrikeGFX   1.0    0.0   \n",
       "1747502   conspiracy    news  1.455682e+09  goober_boobz   1.0    0.0   \n",
       "\n",
       "         authorlinkkarma  authorkarma  authorisgold  \n",
       "1767257           2794.0       1807.0           0.0  \n",
       "237144             276.0       2542.0           0.0  \n",
       "1747502            190.0       2997.0           0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Records per meta subreddit (top 10):')\n",
    "print(\n",
    "    combined_df['meta']\n",
    "    .value_counts()\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "combined_df.sample(3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbaa5d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 – configuration + helpers for term detection\n",
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Detection window settings\n",
    "RECENT_WINDOW_DAYS = 7          # \"now\" window we care about\n",
    "BASELINE_WINDOW_DAYS = 21       # compare against the previous few weeks\n",
    "FRESHNESS_DAYS = 35             # only keep terms first seen in this period\n",
    "MIN_RECENT_FREQ = 20            # minimum token occurrences in recent window\n",
    "MAX_BASELINE_FREQ = 50          # treat anything more frequent as \"not novel\"\n",
    "TOP_K_TERMS = 20                # final number of candidates to keep\n",
    "\n",
    "token_pattern = re.compile(r\"[a-zA-Z][a-zA-Z0-9'#_+-]{1,24}\")\n",
    "\n",
    "STOPWORDS = {\n",
    "    \"the\",\"and\",\"you\",\"that\",\"with\",\"this\",\"have\",\"your\",\"from\",\"they\",\"them\",\"what\",\n",
    "    \"when\",\"were\",\"would\",\"there\",\"could\",\"should\",\"about\",\"because\",\"their\",\"just\",\n",
    "    \"like\",\"cant\",\"dont\",\"doesnt\",\"im\",\"ive\",\"ill\",\"lets\",\"was\",\"for\",\"are\",\"but\",\n",
    "}\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    return \" \".join(text.lower().split())\n",
    "\n",
    "def extract_terms(text: str) -> list[str]:\n",
    "    tokens = [t for t in token_pattern.findall(text.lower()) if t not in STOPWORDS]\n",
    "    bigrams = [\n",
    "        f\"{a} {b}\"\n",
    "        for a, b in zip(tokens, tokens[1:])\n",
    "        if a not in STOPWORDS or b not in STOPWORDS\n",
    "    ]\n",
    "    return tokens + bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76c532c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploded 120,348,886 term uses into 646,888 unique terms across 7 meta communities.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meta</th>\n",
       "      <th>event_dt</th>\n",
       "      <th>terms</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>2015-09-14 00:00:00+00:00</td>\n",
       "      <td>accepting</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>2015-09-14 00:00:00+00:00</td>\n",
       "      <td>accepting new</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>2015-09-14 00:00:00+00:00</td>\n",
       "      <td>account</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>2015-09-14 00:00:00+00:00</td>\n",
       "      <td>account is</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>2015-09-14 00:00:00+00:00</td>\n",
       "      <td>adhere</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            meta                  event_dt          terms  freq\n",
       "0  entertainment 2015-09-14 00:00:00+00:00      accepting    50\n",
       "1  entertainment 2015-09-14 00:00:00+00:00  accepting new    50\n",
       "2  entertainment 2015-09-14 00:00:00+00:00        account    50\n",
       "3  entertainment 2015-09-14 00:00:00+00:00     account is    50\n",
       "4  entertainment 2015-09-14 00:00:00+00:00         adhere    50"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 2 – explode posts into daily term counts\n",
    "term_df = (\n",
    "    combined_df\n",
    "    .assign(\n",
    "        event_dt=pd.to_datetime(combined_df[\"time\"], unit=\"s\", utc=True).dt.floor(\"D\"),\n",
    "        text_norm=combined_df[\"text\"].fillna(\"\").map(normalize_text),\n",
    "    )\n",
    "    .loc[:, [\"meta\", \"event_dt\", \"text_norm\"]]\n",
    ")\n",
    "\n",
    "term_df[\"terms\"] = term_df[\"text_norm\"].map(extract_terms)\n",
    "term_df = term_df.explode(\"terms\").dropna(subset=[\"terms\"])\n",
    "\n",
    "daily_counts = (\n",
    "    term_df.groupby([\"meta\", \"event_dt\", \"terms\"])\n",
    "    .size()\n",
    "    .rename(\"freq\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(f\"Exploded {len(term_df):,} term uses into \"\n",
    "      f\"{daily_counts['terms'].nunique():,} unique terms across \"\n",
    "      f\"{daily_counts['meta'].nunique()} meta communities.\")\n",
    "daily_counts.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1efb987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meta</th>\n",
       "      <th>terms</th>\n",
       "      <th>recent_freq</th>\n",
       "      <th>baseline_freq</th>\n",
       "      <th>growth_ratio</th>\n",
       "      <th>novelty_score</th>\n",
       "      <th>first_seen</th>\n",
       "      <th>baseline_window</th>\n",
       "      <th>analysis_window</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>236471</th>\n",
       "      <td>humor</td>\n",
       "      <td>bernie</td>\n",
       "      <td>191997</td>\n",
       "      <td>0.0</td>\n",
       "      <td>191998.0</td>\n",
       "      <td>3.686304e+10</td>\n",
       "      <td>2016-02-12 00:00:00+00:00</td>\n",
       "      <td>2016-01-21 → 2016-02-10</td>\n",
       "      <td>2016-02-11 → 2016-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290422</th>\n",
       "      <td>humor</td>\n",
       "      <td>sanders</td>\n",
       "      <td>188700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>188701.0</td>\n",
       "      <td>3.560788e+10</td>\n",
       "      <td>2016-02-13 00:00:00+00:00</td>\n",
       "      <td>2016-01-21 → 2016-02-10</td>\n",
       "      <td>2016-02-11 → 2016-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236498</th>\n",
       "      <td>humor</td>\n",
       "      <td>bernie sanders</td>\n",
       "      <td>187600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>187601.0</td>\n",
       "      <td>3.519395e+10</td>\n",
       "      <td>2016-02-13 00:00:00+00:00</td>\n",
       "      <td>2016-01-21 → 2016-02-10</td>\n",
       "      <td>2016-02-11 → 2016-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290428</th>\n",
       "      <td>humor</td>\n",
       "      <td>sanders bernie</td>\n",
       "      <td>186150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>186151.0</td>\n",
       "      <td>3.465201e+10</td>\n",
       "      <td>2016-02-16 00:00:00+00:00</td>\n",
       "      <td>2016-01-21 → 2016-02-10</td>\n",
       "      <td>2016-02-11 → 2016-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264018</th>\n",
       "      <td>humor</td>\n",
       "      <td>is</td>\n",
       "      <td>178048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>178049.0</td>\n",
       "      <td>3.170127e+10</td>\n",
       "      <td>2016-02-11 00:00:00+00:00</td>\n",
       "      <td>2016-01-21 → 2016-02-10</td>\n",
       "      <td>2016-02-11 → 2016-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262139</th>\n",
       "      <td>humor</td>\n",
       "      <td>in</td>\n",
       "      <td>95667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>95668.0</td>\n",
       "      <td>9.152271e+09</td>\n",
       "      <td>2016-02-12 00:00:00+00:00</td>\n",
       "      <td>2016-01-21 → 2016-02-10</td>\n",
       "      <td>2016-02-11 → 2016-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266526</th>\n",
       "      <td>humor</td>\n",
       "      <td>kanye</td>\n",
       "      <td>77245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>77246.0</td>\n",
       "      <td>5.966867e+09</td>\n",
       "      <td>2016-02-14 00:00:00+00:00</td>\n",
       "      <td>2016-01-21 → 2016-02-10</td>\n",
       "      <td>2016-02-11 → 2016-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614252</th>\n",
       "      <td>news</td>\n",
       "      <td>he</td>\n",
       "      <td>77094</td>\n",
       "      <td>0.0</td>\n",
       "      <td>77095.0</td>\n",
       "      <td>5.943562e+09</td>\n",
       "      <td>2016-02-11 00:00:00+00:00</td>\n",
       "      <td>2016-01-21 → 2016-02-10</td>\n",
       "      <td>2016-02-11 → 2016-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307323</th>\n",
       "      <td>humor</td>\n",
       "      <td>west</td>\n",
       "      <td>74898</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74899.0</td>\n",
       "      <td>5.609785e+09</td>\n",
       "      <td>2016-02-14 00:00:00+00:00</td>\n",
       "      <td>2016-01-21 → 2016-02-10</td>\n",
       "      <td>2016-02-11 → 2016-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266565</th>\n",
       "      <td>humor</td>\n",
       "      <td>kanye west</td>\n",
       "      <td>74348</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74349.0</td>\n",
       "      <td>5.527699e+09</td>\n",
       "      <td>2016-02-15 00:00:00+00:00</td>\n",
       "      <td>2016-01-21 → 2016-02-10</td>\n",
       "      <td>2016-02-11 → 2016-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272166</th>\n",
       "      <td>humor</td>\n",
       "      <td>mentally</td>\n",
       "      <td>74298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74299.0</td>\n",
       "      <td>5.520267e+09</td>\n",
       "      <td>2016-02-16 00:00:00+00:00</td>\n",
       "      <td>2016-01-21 → 2016-02-10</td>\n",
       "      <td>2016-02-11 → 2016-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264461</th>\n",
       "      <td>humor</td>\n",
       "      <td>is mentally</td>\n",
       "      <td>74050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74051.0</td>\n",
       "      <td>5.483477e+09</td>\n",
       "      <td>2016-02-16 00:00:00+00:00</td>\n",
       "      <td>2016-01-21 → 2016-02-10</td>\n",
       "      <td>2016-02-11 → 2016-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307329</th>\n",
       "      <td>humor</td>\n",
       "      <td>west is</td>\n",
       "      <td>74050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74051.0</td>\n",
       "      <td>5.483477e+09</td>\n",
       "      <td>2016-02-16 00:00:00+00:00</td>\n",
       "      <td>2016-01-21 → 2016-02-10</td>\n",
       "      <td>2016-02-11 → 2016-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272170</th>\n",
       "      <td>humor</td>\n",
       "      <td>mentally kanye</td>\n",
       "      <td>73600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73601.0</td>\n",
       "      <td>5.417034e+09</td>\n",
       "      <td>2016-02-16 00:00:00+00:00</td>\n",
       "      <td>2016-01-21 → 2016-02-10</td>\n",
       "      <td>2016-02-11 → 2016-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37829</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>he</td>\n",
       "      <td>69193</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69194.0</td>\n",
       "      <td>4.787740e+09</td>\n",
       "      <td>2016-02-12 00:00:00+00:00</td>\n",
       "      <td>2016-01-21 → 2016-02-10</td>\n",
       "      <td>2016-02-11 → 2016-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282147</th>\n",
       "      <td>humor</td>\n",
       "      <td>pablo</td>\n",
       "      <td>67649</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67650.0</td>\n",
       "      <td>4.576455e+09</td>\n",
       "      <td>2016-02-15 00:00:00+00:00</td>\n",
       "      <td>2016-01-21 → 2016-02-10</td>\n",
       "      <td>2016-02-11 → 2016-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282149</th>\n",
       "      <td>humor</td>\n",
       "      <td>pablo pablo</td>\n",
       "      <td>67450</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67451.0</td>\n",
       "      <td>4.549570e+09</td>\n",
       "      <td>2016-02-16 00:00:00+00:00</td>\n",
       "      <td>2016-01-21 → 2016-02-10</td>\n",
       "      <td>2016-02-11 → 2016-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236462</th>\n",
       "      <td>humor</td>\n",
       "      <td>ber</td>\n",
       "      <td>62550</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62551.0</td>\n",
       "      <td>3.912565e+09</td>\n",
       "      <td>2016-02-14 00:00:00+00:00</td>\n",
       "      <td>2016-01-21 → 2016-02-10</td>\n",
       "      <td>2016-02-11 → 2016-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236463</th>\n",
       "      <td>humor</td>\n",
       "      <td>ber nie</td>\n",
       "      <td>62500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62501.0</td>\n",
       "      <td>3.906312e+09</td>\n",
       "      <td>2016-02-17 00:00:00+00:00</td>\n",
       "      <td>2016-01-21 → 2016-02-10</td>\n",
       "      <td>2016-02-11 → 2016-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275737</th>\n",
       "      <td>humor</td>\n",
       "      <td>nie</td>\n",
       "      <td>62500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62501.0</td>\n",
       "      <td>3.906312e+09</td>\n",
       "      <td>2016-02-17 00:00:00+00:00</td>\n",
       "      <td>2016-01-21 → 2016-02-10</td>\n",
       "      <td>2016-02-11 → 2016-02-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 meta           terms  recent_freq  baseline_freq  \\\n",
       "236471          humor          bernie       191997            0.0   \n",
       "290422          humor         sanders       188700            0.0   \n",
       "236498          humor  bernie sanders       187600            0.0   \n",
       "290428          humor  sanders bernie       186150            0.0   \n",
       "264018          humor              is       178048            0.0   \n",
       "262139          humor              in        95667            0.0   \n",
       "266526          humor           kanye        77245            0.0   \n",
       "614252           news              he        77094            0.0   \n",
       "307323          humor            west        74898            0.0   \n",
       "266565          humor      kanye west        74348            0.0   \n",
       "272166          humor        mentally        74298            0.0   \n",
       "264461          humor     is mentally        74050            0.0   \n",
       "307329          humor         west is        74050            0.0   \n",
       "272170          humor  mentally kanye        73600            0.0   \n",
       "37829   entertainment              he        69193            0.0   \n",
       "282147          humor           pablo        67649            0.0   \n",
       "282149          humor     pablo pablo        67450            0.0   \n",
       "236462          humor             ber        62550            0.0   \n",
       "236463          humor         ber nie        62500            0.0   \n",
       "275737          humor             nie        62500            0.0   \n",
       "\n",
       "        growth_ratio  novelty_score                first_seen  \\\n",
       "236471      191998.0   3.686304e+10 2016-02-12 00:00:00+00:00   \n",
       "290422      188701.0   3.560788e+10 2016-02-13 00:00:00+00:00   \n",
       "236498      187601.0   3.519395e+10 2016-02-13 00:00:00+00:00   \n",
       "290428      186151.0   3.465201e+10 2016-02-16 00:00:00+00:00   \n",
       "264018      178049.0   3.170127e+10 2016-02-11 00:00:00+00:00   \n",
       "262139       95668.0   9.152271e+09 2016-02-12 00:00:00+00:00   \n",
       "266526       77246.0   5.966867e+09 2016-02-14 00:00:00+00:00   \n",
       "614252       77095.0   5.943562e+09 2016-02-11 00:00:00+00:00   \n",
       "307323       74899.0   5.609785e+09 2016-02-14 00:00:00+00:00   \n",
       "266565       74349.0   5.527699e+09 2016-02-15 00:00:00+00:00   \n",
       "272166       74299.0   5.520267e+09 2016-02-16 00:00:00+00:00   \n",
       "264461       74051.0   5.483477e+09 2016-02-16 00:00:00+00:00   \n",
       "307329       74051.0   5.483477e+09 2016-02-16 00:00:00+00:00   \n",
       "272170       73601.0   5.417034e+09 2016-02-16 00:00:00+00:00   \n",
       "37829        69194.0   4.787740e+09 2016-02-12 00:00:00+00:00   \n",
       "282147       67650.0   4.576455e+09 2016-02-15 00:00:00+00:00   \n",
       "282149       67451.0   4.549570e+09 2016-02-16 00:00:00+00:00   \n",
       "236462       62551.0   3.912565e+09 2016-02-14 00:00:00+00:00   \n",
       "236463       62501.0   3.906312e+09 2016-02-17 00:00:00+00:00   \n",
       "275737       62501.0   3.906312e+09 2016-02-17 00:00:00+00:00   \n",
       "\n",
       "                baseline_window          analysis_window  \n",
       "236471  2016-01-21 → 2016-02-10  2016-02-11 → 2016-02-17  \n",
       "290422  2016-01-21 → 2016-02-10  2016-02-11 → 2016-02-17  \n",
       "236498  2016-01-21 → 2016-02-10  2016-02-11 → 2016-02-17  \n",
       "290428  2016-01-21 → 2016-02-10  2016-02-11 → 2016-02-17  \n",
       "264018  2016-01-21 → 2016-02-10  2016-02-11 → 2016-02-17  \n",
       "262139  2016-01-21 → 2016-02-10  2016-02-11 → 2016-02-17  \n",
       "266526  2016-01-21 → 2016-02-10  2016-02-11 → 2016-02-17  \n",
       "614252  2016-01-21 → 2016-02-10  2016-02-11 → 2016-02-17  \n",
       "307323  2016-01-21 → 2016-02-10  2016-02-11 → 2016-02-17  \n",
       "266565  2016-01-21 → 2016-02-10  2016-02-11 → 2016-02-17  \n",
       "272166  2016-01-21 → 2016-02-10  2016-02-11 → 2016-02-17  \n",
       "264461  2016-01-21 → 2016-02-10  2016-02-11 → 2016-02-17  \n",
       "307329  2016-01-21 → 2016-02-10  2016-02-11 → 2016-02-17  \n",
       "272170  2016-01-21 → 2016-02-10  2016-02-11 → 2016-02-17  \n",
       "37829   2016-01-21 → 2016-02-10  2016-02-11 → 2016-02-17  \n",
       "282147  2016-01-21 → 2016-02-10  2016-02-11 → 2016-02-17  \n",
       "282149  2016-01-21 → 2016-02-10  2016-02-11 → 2016-02-17  \n",
       "236462  2016-01-21 → 2016-02-10  2016-02-11 → 2016-02-17  \n",
       "236463  2016-01-21 → 2016-02-10  2016-02-11 → 2016-02-17  \n",
       "275737  2016-01-21 → 2016-02-10  2016-02-11 → 2016-02-17  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected 20 candidate terms; use this list as input to the SIR + semantic tracking pipeline.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 – novelty scoring + top candidate list\n",
    "analysis_end = daily_counts[\"event_dt\"].max()\n",
    "recent_start = analysis_end - pd.Timedelta(days=RECENT_WINDOW_DAYS - 1)\n",
    "baseline_start = recent_start - pd.Timedelta(days=BASELINE_WINDOW_DAYS)\n",
    "\n",
    "recent_mask = daily_counts[\"event_dt\"].between(recent_start, analysis_end)\n",
    "baseline_mask = daily_counts[\"event_dt\"].between(baseline_start, recent_start - pd.Timedelta(days=1))\n",
    "fresh_mask = daily_counts[\"event_dt\"] >= analysis_end - pd.Timedelta(days=FRESHNESS_DAYS)\n",
    "\n",
    "recent_freq = (\n",
    "    daily_counts.loc[recent_mask]\n",
    "    .groupby([\"meta\", \"terms\"])[\"freq\"]\n",
    "    .sum()\n",
    "    .rename(\"recent_freq\")\n",
    ")\n",
    "\n",
    "baseline_freq = (\n",
    "    daily_counts.loc[baseline_mask]\n",
    "    .groupby([\"meta\", \"terms\"])[\"freq\"]\n",
    "    .sum()\n",
    "    .rename(\"baseline_freq\")\n",
    ")\n",
    "\n",
    "first_seen = (\n",
    "    daily_counts.loc[fresh_mask]\n",
    "    .groupby([\"meta\", \"terms\"])[\"event_dt\"]\n",
    "    .min()\n",
    "    .rename(\"first_seen\")\n",
    ")\n",
    "\n",
    "scored = (\n",
    "    recent_freq\n",
    "    .to_frame()\n",
    "    .join(baseline_freq, how=\"left\")\n",
    "    .join(first_seen, how=\"left\")\n",
    "    .fillna({\"baseline_freq\": 0})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "scored = scored[\n",
    "    (scored[\"recent_freq\"] >= MIN_RECENT_FREQ) &\n",
    "    (scored[\"baseline_freq\"] <= MAX_BASELINE_FREQ) &\n",
    "    (scored[\"first_seen\"].notna())\n",
    "]\n",
    "\n",
    "scored[\"growth_ratio\"] = (scored[\"recent_freq\"] + 1) / (scored[\"baseline_freq\"] + 1)\n",
    "scored[\"novelty_score\"] = scored[\"growth_ratio\"] * scored[\"recent_freq\"]\n",
    "\n",
    "top_terms = (\n",
    "    scored.sort_values(\"novelty_score\", ascending=False)\n",
    "    .head(TOP_K_TERMS)\n",
    "    .assign(\n",
    "        analysis_window=f\"{recent_start.date()} → {analysis_end.date()}\",\n",
    "        baseline_window=f\"{baseline_start.date()} → {(recent_start - pd.Timedelta(days=1)).date()}\",\n",
    "    )\n",
    ")\n",
    "\n",
    "display(top_terms[[\n",
    "    \"meta\", \"terms\", \"recent_freq\", \"baseline_freq\",\n",
    "    \"growth_ratio\", \"novelty_score\", \"first_seen\",\n",
    "    \"baseline_window\", \"analysis_window\",\n",
    "]])\n",
    "\n",
    "print(f\"\\nSelected {len(top_terms)} candidate terms; \"\n",
    "      \"use this list as input to the SIR + semantic tracking pipeline.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833107ae",
   "metadata": {},
   "source": [
    "NOTE: Above is very naive term detection with exact matches to establish baseline results. The below is a more refined version using embedding-based anomaly detection. Further updates required to refine memory use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "863537a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 73.1 MiB for an array with shape (76657067,) and data type bool",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     23\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mtokens\u001b[39m\u001b[33m\"\u001b[39m] = df[\u001b[33m\"\u001b[39m\u001b[33mtext_norm\u001b[39m\u001b[33m\"\u001b[39m].str.findall(token_pattern)\n\u001b[32m     25\u001b[39m token_df = (\n\u001b[32m     26\u001b[39m     df.loc[:, [\u001b[33m\"\u001b[39m\u001b[33mmeta\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mevent_dt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtext_norm\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtokens\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m     27\u001b[39m       .explode(\u001b[33m\"\u001b[39m\u001b[33mtokens\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m       .dropna(subset=[\u001b[33m\"\u001b[39m\u001b[33mtokens\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     29\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m token_df[\u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m] = token_df[\u001b[33m\"\u001b[39m\u001b[33mtokens\u001b[39m\u001b[33m\"\u001b[39m] + \u001b[33m\"\u001b[39m\u001b[33m || \u001b[39m\u001b[33m\"\u001b[39m + \u001b[43mtoken_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext_norm\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mslice\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m240\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m analysis_end = token_df[\u001b[33m\"\u001b[39m\u001b[33mevent_dt\u001b[39m\u001b[33m\"\u001b[39m].max()\n\u001b[32m     33\u001b[39m recent_start = analysis_end - pd.Timedelta(days=RECENT_DAYS - \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namit\\anaconda3\\envs\\viral_linguistics\\Lib\\site-packages\\pandas\\core\\strings\\accessor.py:1906\u001b[39m, in \u001b[36mStringMethods.slice\u001b[39m\u001b[34m(self, start, stop, step)\u001b[39m\n\u001b[32m   1834\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mslice\u001b[39m(\u001b[38;5;28mself\u001b[39m, start=\u001b[38;5;28;01mNone\u001b[39;00m, stop=\u001b[38;5;28;01mNone\u001b[39;00m, step=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1835\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1836\u001b[39m \u001b[33;03m    Slice substrings from each element in the Series or Index.\u001b[39;00m\n\u001b[32m   1837\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1904\u001b[39m \u001b[33;03m    dtype: object\u001b[39;00m\n\u001b[32m   1905\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1906\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_str_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1907\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._wrap_result(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namit\\anaconda3\\envs\\viral_linguistics\\Lib\\site-packages\\pandas\\core\\strings\\object_array.py:343\u001b[39m, in \u001b[36mObjectStringArrayMixin._str_slice\u001b[39m\u001b[34m(self, start, stop, step)\u001b[39m\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_str_slice\u001b[39m(\u001b[38;5;28mself\u001b[39m, start=\u001b[38;5;28;01mNone\u001b[39;00m, stop=\u001b[38;5;28;01mNone\u001b[39;00m, step=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    342\u001b[39m     obj = \u001b[38;5;28mslice\u001b[39m(start, stop, step)\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_str_map\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namit\\anaconda3\\envs\\viral_linguistics\\Lib\\site-packages\\pandas\\core\\strings\\object_array.py:82\u001b[39m, in \u001b[36mObjectStringArrayMixin._str_map\u001b[39m\u001b[34m(self, f, na_value, dtype, convert)\u001b[39m\n\u001b[32m     80\u001b[39m map_convert = convert \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.all(mask)\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     result = \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_convert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m     84\u001b[39m     \u001b[38;5;66;03m# Reraise the exception if callable `f` got wrong number of args.\u001b[39;00m\n\u001b[32m     85\u001b[39m     \u001b[38;5;66;03m# The user may want to be warned by this, instead of getting NaN\u001b[39;00m\n\u001b[32m     86\u001b[39m     p_err = (\n\u001b[32m     87\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m((takes)|(missing)) (?(2)from \u001b[39m\u001b[33m\\\u001b[39m\u001b[33md+ to )?\u001b[39m\u001b[33m\\\u001b[39m\u001b[33md+ \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     88\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(?(3)required )positional arguments?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     89\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2930\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer_mask\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2560\u001b[39m, in \u001b[36mpandas._libs.lib.maybe_convert_objects\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namit\\anaconda3\\envs\\viral_linguistics\\Lib\\site-packages\\numpy\\_core\\numeric.py:386\u001b[39m, in \u001b[36mfull\u001b[39m\u001b[34m(shape, fill_value, dtype, order, device, like)\u001b[39m\n\u001b[32m    384\u001b[39m     fill_value = asarray(fill_value)\n\u001b[32m    385\u001b[39m     dtype = fill_value.dtype\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m a = \u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    387\u001b[39m multiarray.copyto(a, fill_value, casting=\u001b[33m'\u001b[39m\u001b[33munsafe\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 73.1 MiB for an array with shape (76657067,) and data type bool"
     ]
    }
   ],
   "source": [
    "# Cell 2 – configure windows + build context datasets\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "BASELINE_DAYS = 21        # training window for \"normal\" language\n",
    "RECENT_DAYS = 7           # window we want to flag anomalies in\n",
    "FRESHNESS_DAYS = 35       # only examine tokens first seen recently\n",
    "MIN_RECENT_USES = 10\n",
    "MAX_CONTEXTS_PER_TOKEN = 5\n",
    "MAX_BASELINE_CONTEXTS = 20000\n",
    "MAX_RECENT_CONTEXTS = 6000\n",
    "\n",
    "token_pattern = re.compile(r\"[a-zA-Z][a-zA-Z0-9'#_+-]{1,24}\")\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", text.lower().strip())\n",
    "\n",
    "df = combined_df.copy()\n",
    "df[\"event_dt\"] = pd.to_datetime(df[\"time\"], unit=\"s\", utc=True).dt.floor(\"D\")\n",
    "df[\"text_norm\"] = df[\"text\"].fillna(\"\").map(normalize_text)\n",
    "df[\"tokens\"] = df[\"text_norm\"].str.findall(token_pattern)\n",
    "\n",
    "token_df = (\n",
    "    df.loc[:, [\"meta\", \"event_dt\", \"text_norm\", \"tokens\"]]\n",
    "      .explode(\"tokens\")\n",
    "      .dropna(subset=[\"tokens\"])\n",
    ")\n",
    "token_df[\"context\"] = token_df[\"tokens\"] + \" || \" + token_df[\"text_norm\"].str.slice(0, 240)\n",
    "\n",
    "analysis_end = token_df[\"event_dt\"].max()\n",
    "recent_start = analysis_end - pd.Timedelta(days=RECENT_DAYS - 1)\n",
    "baseline_start = recent_start - pd.Timedelta(days=BASELINE_DAYS)\n",
    "fresh_cutoff = analysis_end - pd.Timedelta(days=FRESHNESS_DAYS)\n",
    "\n",
    "baseline_df = token_df[\n",
    "    token_df[\"event_dt\"].between(baseline_start, recent_start - pd.Timedelta(days=1))\n",
    "]\n",
    "recent_df = token_df[\n",
    "    token_df[\"event_dt\"].between(recent_start, analysis_end) &\n",
    "    token_df[\"event_dt\"].ge(fresh_cutoff)\n",
    "]\n",
    "\n",
    "def cap_contexts(group, max_rows):\n",
    "    if len(group) <= max_rows:\n",
    "        return group\n",
    "    return group.sample(max_rows, random_state=42)\n",
    "\n",
    "baseline_df = (\n",
    "    baseline_df.groupby(\"tokens\", group_keys=False)\n",
    "               .apply(cap_contexts, MAX_CONTEXTS_PER_TOKEN)\n",
    "               .sample(min(len(baseline_df), MAX_BASELINE_CONTEXTS), random_state=42)\n",
    "               .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "recent_df = (\n",
    "    recent_df.groupby(\"tokens\", group_keys=False)\n",
    "             .apply(cap_contexts, MAX_CONTEXTS_PER_TOKEN)\n",
    "             .sample(min(len(recent_df), MAX_RECENT_CONTEXTS), random_state=42)\n",
    "             .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f\"Baseline contexts: {len(baseline_df):,} across {baseline_df['tokens'].nunique():,} tokens\")\n",
    "print(f\"Recent contexts:   {len(recent_df):,} across {recent_df['tokens'].nunique():,} tokens \"\n",
    "      f\"({recent_start.date()} → {analysis_end.date()})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617d9951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 – embed contexts, run anomaly detection, and list top emerging terms\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "baseline_emb = model.encode(\n",
    "    baseline_df[\"context\"].tolist(),\n",
    "    batch_size=256,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True,\n",
    ")\n",
    "\n",
    "recent_emb = model.encode(\n",
    "    recent_df[\"context\"].tolist(),\n",
    "    batch_size=256,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True,\n",
    ")\n",
    "\n",
    "iso = IsolationForest(\n",
    "    n_estimators=256,\n",
    "    contamination=0.05,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "iso.fit(baseline_emb)\n",
    "\n",
    "recent_scores = -iso.score_samples(recent_emb)  # higher = more anomalous\n",
    "recent_scored = recent_df.assign(anomaly_score=recent_scores)\n",
    "\n",
    "agg = (\n",
    "    recent_scored.groupby(\"tokens\")\n",
    "    .agg(\n",
    "        mean_anomaly=(\"anomaly_score\", \"mean\"),\n",
    "        recent_uses=(\"tokens\", \"size\"),\n",
    "        first_seen=(\"event_dt\", \"min\"),\n",
    "        metas=(\"meta\", lambda x: \", \".join(pd.Series(x).value_counts().head(2).index)),\n",
    "    )\n",
    "    .query(\"recent_uses >= @MIN_RECENT_USES\")\n",
    "    .sort_values(\"mean_anomaly\", ascending=False)\n",
    ")\n",
    "\n",
    "top_terms = (\n",
    "    agg.head(20)\n",
    "       .assign(\n",
    "           recent_window=f\"{recent_start.date()} → {analysis_end.date()}\",\n",
    "           baseline_window=f\"{baseline_start.date()} → {(recent_start - pd.Timedelta(days=1)).date()}\",\n",
    "       )\n",
    "       .reset_index(names=\"term\")\n",
    ")\n",
    "\n",
    "display(top_terms[[\n",
    "    \"term\", \"mean_anomaly\", \"recent_uses\", \"first_seen\",\n",
    "    \"metas\", \"baseline_window\", \"recent_window\",\n",
    "]])\n",
    "\n",
    "print(f\"\\nIdentified {len(top_terms)} embedding-level anomaly terms; \"\n",
    "      \"feed their aggregated counts into the SIR pipeline next.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "viral_linguistics_kernel",
   "language": "python",
   "name": "viral_linguistics_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
