{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4399ceb4",
   "metadata": {},
   "source": [
    "## Reddit dataset consolidation\n",
    "\n",
    "This notebook gathers every CSV inside `Reddit Dataset/` (except the large `kaggle_RC_2019-05.csv`) and loads them with the correct headers provided in `headers.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "933926a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T04:50:13.711539Z",
     "iopub.status.busy": "2025-11-09T04:50:13.711095Z",
     "iopub.status.idle": "2025-11-09T04:50:14.066467Z",
     "shell.execute_reply": "2025-11-09T04:50:14.065153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 49 subreddit CSV files to combine.\n",
      "Meta groups: ['entertainment', 'gaming', 'humor', 'learning', 'lifestyle', 'news', 'television']\n",
      "First 5 files: ['entertainment_comicbooks.csv', 'entertainment_harrypotter.csv', 'entertainment_movies.csv', 'entertainment_music.csv', 'entertainment_starwars.csv']\n",
      "Last 5 files: ['television_gameofthrones.csv', 'television_himym.csv', 'television_mylittlepony.csv', 'television_startrek.csv', 'television_thewalkingdead.csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_969812/1839189031.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = Path(\"Reddit Dataset\")\n",
    "EXCLUDE_FILES = {\"kaggle_RC_2019-05.csv\"}  # huge generic dump; skip to keep things tractable\n",
    "\n",
    "COLUMN_NAMES = [\n",
    "    \"text\",\n",
    "    \"id\",\n",
    "    \"subreddit\",\n",
    "    \"meta\",\n",
    "    \"time\",\n",
    "    \"author\",\n",
    "    \"ups\",\n",
    "    \"downs\",\n",
    "    \"authorlinkkarma\",\n",
    "    \"authorkarma\",\n",
    "    \"authorisgold\",\n",
    "]\n",
    "\n",
    "csv_paths = sorted(\n",
    "    path for path in DATA_DIR.glob(\"*.csv\") if path.name not in EXCLUDE_FILES\n",
    ")\n",
    "meta_groups = sorted({path.stem.split(\"_\", 1)[0] for path in csv_paths})\n",
    "\n",
    "print(f\"Found {len(csv_paths)} subreddit CSV files to combine.\")\n",
    "print(\"Meta groups:\", meta_groups)\n",
    "print(\"First 5 files:\", [p.name for p in csv_paths[:5]])\n",
    "print(\"Last 5 files:\", [p.name for p in csv_paths[-5:]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74617f48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T04:50:14.068423Z",
     "iopub.status.busy": "2025-11-09T04:50:14.068111Z",
     "iopub.status.idle": "2025-11-09T04:50:27.170732Z",
     "shell.execute_reply": "2025-11-09T04:50:27.168992Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 1 malformed rows in entertainment_comicbooks.csv\n",
      "Filtered 1 malformed rows in entertainment_harrypotter.csv\n",
      "Filtered 1 malformed rows in entertainment_movies.csv\n",
      "\n",
      "Combined shape: 2,423,702 rows × 11 columns\n",
      "Dropped 3 mismatched rows overall.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>meta</th>\n",
       "      <th>time</th>\n",
       "      <th>author</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>authorlinkkarma</th>\n",
       "      <th>authorkarma</th>\n",
       "      <th>authorisgold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sometimes they have a difference of opinion  s...</td>\n",
       "      <td>d01727e</td>\n",
       "      <td>comicbooks</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>1.455577e+09</td>\n",
       "      <td>TheStealthBox</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>32044.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>try polysuede or felt that is acidfree or pass...</td>\n",
       "      <td>d02fswl</td>\n",
       "      <td>comicbooks</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>1.455661e+09</td>\n",
       "      <td>mrindustrialist</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>take them in to a second hand book store amp  ...</td>\n",
       "      <td>d01qm82</td>\n",
       "      <td>comicbooks</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>1.455615e+09</td>\n",
       "      <td>matthew_lane</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>7710.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a lot of cities have ways of getting comics in...</td>\n",
       "      <td>d01k3vi</td>\n",
       "      <td>comicbooks</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>1.455597e+09</td>\n",
       "      <td>Daiteach</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>439.0</td>\n",
       "      <td>11111.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i m probably in the minority  but even the  wo...</td>\n",
       "      <td>d01km27</td>\n",
       "      <td>comicbooks</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>1.455598e+09</td>\n",
       "      <td>Nejfelt</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>918.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       id   subreddit  \\\n",
       "0  sometimes they have a difference of opinion  s...  d01727e  comicbooks   \n",
       "1  try polysuede or felt that is acidfree or pass...  d02fswl  comicbooks   \n",
       "2  take them in to a second hand book store amp  ...  d01qm82  comicbooks   \n",
       "3  a lot of cities have ways of getting comics in...  d01k3vi  comicbooks   \n",
       "4  i m probably in the minority  but even the  wo...  d01km27  comicbooks   \n",
       "\n",
       "            meta          time           author  ups  downs  authorlinkkarma  \\\n",
       "0  entertainment  1.455577e+09    TheStealthBox  5.0    0.0            208.0   \n",
       "1  entertainment  1.455661e+09  mrindustrialist  1.0    0.0              1.0   \n",
       "2  entertainment  1.455615e+09     matthew_lane  2.0    0.0            250.0   \n",
       "3  entertainment  1.455597e+09         Daiteach  3.0    0.0            439.0   \n",
       "4  entertainment  1.455598e+09          Nejfelt  2.0    0.0            150.0   \n",
       "\n",
       "   authorkarma  authorisgold  \n",
       "0      32044.0           0.0  \n",
       "1         75.0           0.0  \n",
       "2       7710.0           0.0  \n",
       "3      11111.0           0.0  \n",
       "4        918.0           0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = []\n",
    "total_filtered = 0\n",
    "\n",
    "for csv_path in csv_paths:\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Drop exported pandas index + rogue placeholder column if present\n",
    "    df = df.drop(columns=df.columns[0])\n",
    "    if len(df.columns) > len(COLUMN_NAMES):\n",
    "        df = df.drop(columns=df.columns[0])\n",
    "\n",
    "    if len(df.columns) != len(COLUMN_NAMES):\n",
    "        raise ValueError(\n",
    "            f\"Unexpected column count {len(df.columns)} in {csv_path.name}.\"\n",
    "        )\n",
    "\n",
    "    df.columns = COLUMN_NAMES\n",
    "\n",
    "    meta_group = csv_path.stem.split(\"_\", 1)[0]\n",
    "    before = len(df)\n",
    "    df = df[df[\"meta\"] == meta_group]\n",
    "    filtered = before - len(df)\n",
    "    if filtered:\n",
    "        total_filtered += filtered\n",
    "        print(f\"Filtered {filtered} malformed rows in {csv_path.name}\")\n",
    "\n",
    "    frames.append(df)\n",
    "\n",
    "combined_df = pd.concat(frames, ignore_index=True)\n",
    "print(f\"\\nCombined shape: {combined_df.shape[0]:,} rows × {combined_df.shape[1]} columns\")\n",
    "print(f\"Dropped {total_filtered:,} mismatched rows overall.\")\n",
    "\n",
    "combined_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f39abf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T04:50:27.174101Z",
     "iopub.status.busy": "2025-11-09T04:50:27.173703Z",
     "iopub.status.idle": "2025-11-09T04:50:27.503537Z",
     "shell.execute_reply": "2025-11-09T04:50:27.501784Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records per meta subreddit (top 10):\n",
      "meta\n",
      "gaming           428443\n",
      "news             408716\n",
      "lifestyle        384494\n",
      "humor            382197\n",
      "television       321794\n",
      "learning         271179\n",
      "entertainment    226879\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>meta</th>\n",
       "      <th>time</th>\n",
       "      <th>author</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>authorlinkkarma</th>\n",
       "      <th>authorkarma</th>\n",
       "      <th>authorisgold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1767257</th>\n",
       "      <td>i wish this sub would ban dumb shit like this ...</td>\n",
       "      <td>d01yzxb</td>\n",
       "      <td>libertarian</td>\n",
       "      <td>news</td>\n",
       "      <td>1.455638e+09</td>\n",
       "      <td>AlCapone564</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2794.0</td>\n",
       "      <td>1807.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237144</th>\n",
       "      <td>if only mmr could get you attitude</td>\n",
       "      <td>d02kli8</td>\n",
       "      <td>dota2</td>\n",
       "      <td>gaming</td>\n",
       "      <td>1.455668e+09</td>\n",
       "      <td>ShrikeGFX</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>2542.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1747502</th>\n",
       "      <td>so basically you re fucked out of a good job o...</td>\n",
       "      <td>d02tety</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>news</td>\n",
       "      <td>1.455682e+09</td>\n",
       "      <td>goober_boobz</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>2997.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text       id  \\\n",
       "1767257  i wish this sub would ban dumb shit like this ...  d01yzxb   \n",
       "237144                  if only mmr could get you attitude  d02kli8   \n",
       "1747502  so basically you re fucked out of a good job o...  d02tety   \n",
       "\n",
       "           subreddit    meta          time        author   ups  downs  \\\n",
       "1767257  libertarian    news  1.455638e+09   AlCapone564  30.0    0.0   \n",
       "237144         dota2  gaming  1.455668e+09     ShrikeGFX   1.0    0.0   \n",
       "1747502   conspiracy    news  1.455682e+09  goober_boobz   1.0    0.0   \n",
       "\n",
       "         authorlinkkarma  authorkarma  authorisgold  \n",
       "1767257           2794.0       1807.0           0.0  \n",
       "237144             276.0       2542.0           0.0  \n",
       "1747502            190.0       2997.0           0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Records per meta subreddit (top 10):\")\n",
    "print(combined_df[\"meta\"].value_counts().head(10))\n",
    "\n",
    "combined_df.sample(3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "863537a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "RECENT_DAYS = 5               # look for emergence in the most recent 5 days\n",
    "BASELINE_DAYS = 15            # compare against the preceding 15 days\n",
    "FRESHNESS_DAYS = 20           # ignore tokens first seen earlier than this\n",
    "MIN_RECENT_USES = 15          # minimum contexts in the recent window\n",
    "MAX_CONTEXTS_PER_TOKEN = 5\n",
    "MAX_BASELINE_CONTEXTS = 20_000\n",
    "MAX_RECENT_CONTEXTS = 4_000\n",
    "COSINE_DUP_THRESHOLD = 0.90\n",
    "TARGET_TERM_COUNT = 20\n",
    "\n",
    "TOKEN_REGEX = r\"(?P<token>[a-zA-Z][a-zA-Z0-9'#_+\\-]{1,24})\"\n",
    "\n",
    "STOPWORDS = {\n",
    "    \"the\",\"and\",\"you\",\"that\",\"with\",\"this\",\"have\",\"your\",\"from\",\"they\",\"them\",\n",
    "    \"what\",\"when\",\"were\",\"would\",\"there\",\"could\",\"should\",\"about\",\"because\",\n",
    "    \"their\",\"just\",\"like\",\"cant\",\"dont\",\"doesnt\",\"im\",\"ive\",\"ill\",\"lets\",\n",
    "    \"was\",\"for\",\"are\",\"but\",\n",
    "}\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def sample_contexts(df: pd.DataFrame, max_total: int) -> pd.DataFrame:\n",
    "    \"\"\"Limit contexts per token and overall while keeping pandas happy.\"\"\"\n",
    "    sampled = (\n",
    "        df.groupby(\"token\", group_keys=False, sort=False)\n",
    "          .apply(lambda g: g.sample(min(len(g), MAX_CONTEXTS_PER_TOKEN), random_state=42))\n",
    "    )\n",
    "    if len(sampled) > max_total:\n",
    "        sampled = sampled.sample(max_total, random_state=42)\n",
    "    return sampled.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "617d9951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered to 2,183,806 rows within [2016-01-29 → 2016-02-17].\n",
      "Extracted 59,458,099 token-context pairs (60,070 unique tokens).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_969812/4289969927.py:40: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: g.sample(min(len(g), MAX_CONTEXTS_PER_TOKEN), random_state=42))\n",
      "/tmp/ipykernel_969812/4289969927.py:40: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: g.sample(min(len(g), MAX_CONTEXTS_PER_TOKEN), random_state=42))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline contexts: 20,000 (11,041 tokens)\n",
      "Recent contexts:   4,000 (3,905 tokens) [2016-02-13 → 2016-02-17]\n"
     ]
    }
   ],
   "source": [
    "df = combined_df.loc[:, [\"meta\", \"time\", \"text\"]].copy()\n",
    "df[\"event_dt\"] = pd.to_datetime(df[\"time\"], unit=\"s\", utc=True).dt.floor(\"D\")\n",
    "\n",
    "analysis_end = df[\"event_dt\"].max()\n",
    "recent_start = analysis_end - pd.Timedelta(days=RECENT_DAYS - 1)\n",
    "baseline_start = recent_start - pd.Timedelta(days=BASELINE_DAYS)\n",
    "fresh_cutoff = analysis_end - pd.Timedelta(days=FRESHNESS_DAYS)\n",
    "\n",
    "window_mask = df[\"event_dt\"].between(baseline_start, analysis_end)\n",
    "df = df.loc[window_mask].copy()\n",
    "df[\"text_norm\"] = df[\"text\"].fillna(\"\").map(normalize_text)\n",
    "df = df.loc[df[\"text_norm\"].str.len() > 0]\n",
    "\n",
    "print(f\"Filtered to {len(df):,} rows within [{baseline_start.date()} → {analysis_end.date()}].\")\n",
    "\n",
    "token_matches = (\n",
    "    df[\"text_norm\"]\n",
    "    .str.extractall(TOKEN_REGEX)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"level_0\": \"row_id\", \"token\": \"token\"})\n",
    ")\n",
    "\n",
    "token_df = token_matches.merge(\n",
    "    df[[\"meta\", \"event_dt\", \"text_norm\"]],\n",
    "    left_on=\"row_id\",\n",
    "    right_index=True,\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "token_df = token_df.loc[~token_df[\"token\"].isin(STOPWORDS)]\n",
    "token_df[\"context\"] = token_df[\"token\"] + \" || \" + token_df[\"text_norm\"].str.slice(0, 220)\n",
    "token_df = token_df.loc[:, [\"meta\", \"event_dt\", \"token\", \"context\"]]\n",
    "\n",
    "print(f\"Extracted {len(token_df):,} token-context pairs \"\n",
    "      f\"({token_df['token'].nunique():,} unique tokens).\")\n",
    "\n",
    "baseline_df = token_df[\n",
    "    token_df[\"event_dt\"].between(baseline_start, recent_start - pd.Timedelta(days=1))\n",
    "]\n",
    "recent_df = token_df[\n",
    "    token_df[\"event_dt\"].between(recent_start, analysis_end) &\n",
    "    token_df[\"event_dt\"].ge(fresh_cutoff)\n",
    "]\n",
    "\n",
    "baseline_df = sample_contexts(baseline_df, MAX_BASELINE_CONTEXTS)\n",
    "recent_df = sample_contexts(recent_df, MAX_RECENT_CONTEXTS)\n",
    "\n",
    "print(f\"Baseline contexts: {len(baseline_df):,} ({baseline_df['token'].nunique():,} tokens)\")\n",
    "print(f\"Recent contexts:   {len(recent_df):,} ({recent_df['token'].nunique():,} tokens) \"\n",
    "      f\"[{recent_start.date()} → {analysis_end.date()}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee780bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding on CUDA…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 40/40 [00:04<00:00,  9.13it/s]\n",
      "Batches: 100%|██████████| 8/8 [00:00<00:00,  8.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No terms met recent_uses ≥ 15; retrying with ≥ 7.\n",
      "❌ No candidate terms available even after relaxing thresholds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>mean_anomaly</th>\n",
       "      <th>recent_uses</th>\n",
       "      <th>first_seen</th>\n",
       "      <th>metas</th>\n",
       "      <th>example_context</th>\n",
       "      <th>baseline_window</th>\n",
       "      <th>recent_window</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [term, mean_anomaly, recent_uses, first_seen, metas, example_context, baseline_window, recent_window]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Returned 0 candidate terms ready for SIR modeling, semantic tracking, and downstream analysis.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "model = SentenceTransformer(MODEL_NAME, device=DEVICE)\n",
    "print(f\"Embedding on {DEVICE.upper()}…\")\n",
    "\n",
    "baseline_emb = model.encode(\n",
    "    baseline_df[\"context\"].tolist(),\n",
    "    batch_size=512,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True,\n",
    ")\n",
    "\n",
    "recent_emb = model.encode(\n",
    "    recent_df[\"context\"].tolist(),\n",
    "    batch_size=512,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True,\n",
    ")\n",
    "\n",
    "iso = IsolationForest(\n",
    "    n_estimators=256,\n",
    "    contamination=0.05,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "iso.fit(baseline_emb)\n",
    "\n",
    "scores = -iso.score_samples(recent_emb)\n",
    "recent_scored = recent_df.reset_index(drop=True).assign(\n",
    "    anomaly_score=scores,\n",
    "    embedding=list(recent_emb),\n",
    ")\n",
    "\n",
    "def build_stats(min_uses):\n",
    "    term_stats = (\n",
    "        recent_scored.groupby(\"token\")\n",
    "        .agg(\n",
    "            mean_anomaly=(\"anomaly_score\", \"mean\"),\n",
    "            recent_uses=(\"token\", \"size\"),\n",
    "            first_seen=(\"event_dt\", \"min\"),\n",
    "            metas=(\"meta\", lambda x: \", \".join(pd.Series(x).value_counts().head(2).index)),\n",
    "            example_context=(\"context\", \"first\"),\n",
    "        )\n",
    "    )\n",
    "    centroids = (\n",
    "        recent_scored.groupby(\"token\")[\"embedding\"]\n",
    "        .apply(lambda vecs: np.vstack(vecs).mean(axis=0))\n",
    "        .rename(\"centroid\")\n",
    "    )\n",
    "    stats = (\n",
    "        term_stats.join(centroids)\n",
    "        .query(\"recent_uses >= @min_uses\")\n",
    "        .reset_index()\n",
    "        .rename(columns={\"token\": \"term\"})\n",
    "    )\n",
    "    stats[\"novelty_score\"] = stats[\"mean_anomaly\"] * np.log1p(stats[\"recent_uses\"])\n",
    "    return stats.sort_values(\"novelty_score\", ascending=False)\n",
    "\n",
    "stats = build_stats(MIN_RECENT_USES)\n",
    "if stats.empty:\n",
    "    relaxed_min = max(3, MIN_RECENT_USES // 2)\n",
    "    print(f\"⚠️ No terms met recent_uses ≥ {MIN_RECENT_USES}; retrying with ≥ {relaxed_min}.\")\n",
    "    stats = build_stats(relaxed_min)\n",
    "\n",
    "if stats.empty:\n",
    "    print(\"❌ No candidate terms available even after relaxing thresholds.\")\n",
    "    candidates = pd.DataFrame(columns=[\n",
    "        \"term\",\"mean_anomaly\",\"recent_uses\",\"first_seen\",\n",
    "        \"metas\",\"example_context\",\"baseline_window\",\"recent_window\"\n",
    "    ])\n",
    "else:\n",
    "    def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:\n",
    "        return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-9))\n",
    "\n",
    "    selected = []\n",
    "    for _, row in stats.iterrows():\n",
    "        emb = row[\"centroid\"]\n",
    "        if all(cosine_sim(emb, sel[\"centroid\"]) < COSINE_DUP_THRESHOLD for sel in selected):\n",
    "            selected.append(row)\n",
    "        if len(selected) == TARGET_TERM_COUNT:\n",
    "            break\n",
    "\n",
    "    if not selected:\n",
    "        print(\"⚠️ No terms passed the uniqueness filter; falling back to top-ranked terms.\")\n",
    "        selected = [row for _, row in stats.head(TARGET_TERM_COUNT).iterrows()]\n",
    "\n",
    "    candidates = pd.DataFrame(selected).reset_index(drop=True)\n",
    "    candidates[\"baseline_window\"] = f\"{baseline_start.date()} → {(recent_start - pd.Timedelta(days=1)).date()}\"\n",
    "    candidates[\"recent_window\"] = f\"{recent_start.date()} → {analysis_end.date()}\"\n",
    "    candidates = candidates.drop(columns=\"centroid\")\n",
    "\n",
    "display(candidates[[\n",
    "    \"term\",\"mean_anomaly\",\"recent_uses\",\"first_seen\",\n",
    "    \"metas\",\"example_context\",\"baseline_window\",\"recent_window\"\n",
    "]])\n",
    "\n",
    "print(f\"\\nReturned {len(candidates)} candidate terms ready for SIR modeling, \"\n",
    "      \"semantic tracking, and downstream analysis.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4649fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
